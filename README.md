<div align="center">

# ⚡️Target-Driven Distillation⚡️

[![arXiv](https://img.shields.io/badge/arXiv-coming%20soon-b31b1b.svg?logo=arxiv)](https://arxiv.org)
[![Hugging Face Models](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Models-blue)](https://huggingface.co/RedAIGC/TDD)

</div>

Target-Driven Distillation (TDD) is a state-of-the-art consistency distillation model that largely accelerates the inference processes of diffusion models. Using its delicate strategies of *target timestep selection* and *decoupled guidance*, models distilled by TDD can generated highly detailed images with only a few steps.

Images below are samples generated by TDD-distilled SDXL, with only 4--8 steps.

<div align="center">
  <img src="assets/teaser.jpg" alt="teaser" style="zoom:80%;" />
</div>



## News

- **Aug. 22, 2024**: Project in progress, comming soon.


## Introduction

Target-Driven Distillation (TDD) features three key designs, that differ from previous consistency distillation methods.
1. **TDD adopts a delicate selection strategy of target timesteps, increasing the training efficiency.** Specifically, it first chooses from a predefined set of equidistant denoising schedules (*e.g.* 4--8 steps), then adds a stochatic offset to accomodate non-deterministic sampling (*e.g.* $\gamma$-sampling).
2. **TDD utilizes decoupled guidances during training, making itself open to post-tuning on guidance scale during inference periods.** Specifically, it replaces a portion of the text conditions with unconditional (*i.e.* empty) prompts, in order to align with the standard training process using CFG.
3. **TDD can be optionally equipped with non-equidistant sampling and x0 clipping, enabling a more flexible and accurate way for image sampling.**

<div align="center">
  <img src="assets/tdd_overview.jpg" alt="overview" width="500" />
</div>

Images below are samples generated by SDXL models distilled by mainstream consistency distillation methods LCM, PCM, TCD, and our TDD, from the same seeds.

<div align="center">
  <img src="assets/compare.jpg" alt="comparison" style="zoom:80%;" />
</div>


For details of TDD, please refer to our paper
> **Target-Driven Distillation: Consistency Distillation with Target Timestep Selection and Decoupled Guidance**<br>
> Cunzheng Wang, Ziyuan Guo, Yuxuan Duan, Huaxia Li, Nemo Chen, Xu Tang, Yao Hu<br>
>
> [![arXiv](https://img.shields.io/badge/arXiv-coming%20soon-b31b1b.svg?logo=arxiv)](https://arxiv.org)<br>



## Concact, Collaboration, and Citation

If you have any questions about the code, please do not hesitate to contact me!

Email: polu@xiaohongshu.com

If you find TDD helpful to your research, please cite our paper:
```
@article{Wang2024TDD,
  title     = {Target-Driven Distillation: Consistency Distillation with Target Timestep Selection and Decoupled Guidance},
  author    = {Cunzheng Wang and Ziyuan Guo and Yuxuan Duan and Huaxia Li and Nemo Chen and Xu Tang and Yao Hu},
  journal   = {arXiv preprint arXiv:xxxx.xxxxx},
  year      = {2024}
}
```
